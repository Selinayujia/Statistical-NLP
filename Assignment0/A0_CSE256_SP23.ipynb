{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgNZTjrhcHa0"
   },
   "source": [
    "# CSE 256: Statistical NLP UCSD: Assignment 0\n",
    "\n",
    "## PyTorch + Paper Reading (60 points)\n",
    "### <font color='blue'> Due: Friday April 14, 2023 at  10pm </font>\n",
    "\n",
    "\n",
    "#### IMPORTANT: After copying this notebook to your Google Drive, paste a link to it below. To get a publicly-accessible link, click the *Share* button at the top right, then click \"Get shareable link\" and copy the link. If you fail to do this, you will receive no credit for this assignment!\n",
    "#### <font color=\"red\">Link: paste your link here:  </font>\n",
    "\n",
    "\n",
    "---\n",
    " \n",
    "- All cells should run almost instantly. If they are taking long, you have made a mistake.\n",
    "\n",
    "- Submission Instructions are located at the bottom of the notebook.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y9OxWy-CYlp4"
   },
   "source": [
    "##Question 1 (10 points)\n",
    "In morden NLP, we represent words with low-dimensional vectors also called *embeddings*. We use these embeddings to compute a vector representation $\\boldsymbol{x}$ of a given prefix (a sequence of words). In the below cell, we use [PyTorch](https://pytorch.org), a machine learning framework, to explore this setup. We provide embeddings for the prefix \"Alice talked to\"; your job is to combine them into a single vector representation $\\boldsymbol{x}$ using [element-wise vector addition](https://ml-cheatsheet.readthedocs.io/en/latest/linear_algebra.html#elementwise-operations). \n",
    "\n",
    "*TIP: if you're finding the PyTorch coding problems difficult, you may want to run through [the 60 minutes blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)!*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Su_j1JY1QG5f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix embedding tensor size:  torch.Size([3, 10])\n",
      "tensor([[-1.1258, -1.1524, -0.2506, -0.4339,  0.8487,  0.6920, -0.3160, -2.1152,\n",
      "          0.3223, -1.2633],\n",
      "        [ 0.3500,  0.3081,  0.1198,  1.2377,  1.1168, -0.2473, -1.3527, -1.6959,\n",
      "          0.5667,  0.7935],\n",
      "        [ 0.5988, -1.5551, -0.3414,  1.8530,  0.7502, -0.5855, -0.1734,  0.1835,\n",
      "          1.3894,  1.5863]], grad_fn=<EmbeddingBackward0>)\n",
      "embedding sum:  tensor([-0.1770, -2.3993, -0.4721,  2.6568,  2.7157, -0.1408, -1.8421, -3.6277,\n",
      "         2.2783,  1.1165], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "prefix = 'Alice talked to'\n",
    "\n",
    "# spend some time understanding this code / reading relevant documentation! \n",
    "# this is a toy problem with a 5 word vocabulary and 10-d embeddings\n",
    "embeddings = torch.nn.Embedding(num_embeddings=5, embedding_dim=10)\n",
    "vocab = {'Alice':0, 'talked':1, 'to':2, 'Bob':3, '.':4}\n",
    "\n",
    "# we need to encode our prefix as integer indices (not words) that index \n",
    "# into the embeddings matrix. the below line accomplishes this.\n",
    "# note that PyTorch inputs are always Tensor objects, so we need\n",
    "# to create a LongTensor out of our list of indices first.\n",
    "indices = torch.LongTensor([vocab[w] for w in prefix.split()])\n",
    "prefix_embs = embeddings(indices)\n",
    "print('prefix embedding tensor size: ', prefix_embs.size())\n",
    "\n",
    "# okay! we now have three embeddings corresponding to each of the three\n",
    "# words in the prefix. write some code that adds them element-wise to obtain\n",
    "# a representation of the prefix! store your answer in a variable named \"x\".\n",
    "\n",
    "print(prefix_embs)\n",
    "### YOUR CODE HERE!\n",
    "x = torch.zeros(10)\n",
    "x = torch.sum(prefix_embs,  dim=0)\n",
    "\n",
    "\n",
    "### DO NOT MODIFY THE LINE BELOW\n",
    "print('embedding sum: ', x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F41LYDeoZPYI"
   },
   "source": [
    "##Question 2 (5 points)\n",
    "In practice, we do not use element-wise addition to combine the different word embeddings in the prefix into a single vector representation (a process called *composition*). What is a major issue with approach as a composition function? \n",
    "\n",
    "Answer: Using element-wise addition to combine word embeddings fails to capture both the syntax and semantics of the words in the prefix. The prefix \"not happy,\" the embeddings of \"not\" and \"happy\" may have opposite directions, which will result in a composite vector that does not accurately represent the meaning of the prefix. Furthermore, element-wise addition can produce vectors with magnitudes that are too high or too low, which can make them challenging to use in later tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MaDZekXs0C7"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here (2-3 sentences) </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-xGz2_cZVs7"
   },
   "source": [
    "##Question 3 (10 points)\n",
    "One very important function in current NLP (and for essentially every task we'll look at) is the [softmax](https://pytorch.org/docs/master/nn.functional.html#softmax), which is defined over an $n$-dimensional vector $<x_1, x_2, \\dots, x_n>$ as $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{1 \\leq j \\leq n} e^{x_j}}$. Let's say we have our prefix representation $\\boldsymbol{x}$ from before. We can use the softmax function, along with a linear projection using a matrix $W$, to go from $\\boldsymbol{x}$ to a probability distribution $p$ over the next word: $p = \\text{softmax}(W\\boldsymbol{x}). $Let's explore this in the code cell below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "mClmHIeowL4V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability distribution tensor([0.0102, 0.0526, 0.0056, 0.9069, 0.0247], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# remember, our goal is to produce a probability distribution over the \n",
    "# next word, conditioned on the prefix representation x. This distribution\n",
    "# is thus over the entire vocabulary (i.e., it is a 5-dimensional vector).\n",
    "# take a look at the dimensionality of x, and you'll notice that it is a \n",
    "# 10-dimensional vector. first, we need to **project** this representation\n",
    "# down to 5-d. We'll do this using the below matrix:\n",
    "\n",
    "W = torch.rand(10, 5)\n",
    "\n",
    "# use this matrix to project x to a 5-d space, and then\n",
    "# use the softmax function to convert it to a probability distribution.\n",
    "# this will involve using PyTorch to compute a matrix/vector product.\n",
    "# look through the documentation if you're confused (torch.nn.functional.softmax)\n",
    "# please store your final probability distribution in the \"probs\" variable.\n",
    "\n",
    "### YOUR CODE HERE\n",
    "probs = torch.nn.functional.softmax(dim=0, input=torch.matmul(x, W))\n",
    "\n",
    "\n",
    "### DO NOT MODIFY THE BELOW LINE!\n",
    "print('probability distribution', probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOlrqnSqZ3H8"
   },
   "source": [
    "##Question 4 (15 points)\n",
    "So far, we have looked at just a single prefix (\"Alice talked to\"). In practice, it is common for us to compute many prefixes in one computation, as this enables us to take advantage of GPU parallelism and also obtain better gradient approximations (we'll talk more about the latter point later). This is called *batching*, where each prefix is an example in a larger batch. Here, you'll redo the computations from the previous cells, but instead of having one prefix, you'll have a batch of two prefixes. The final output of this cell should be a 2x5 matrix that contains two probability distributions, one for each prefix. **NOTE: YOU WILL LOSE POINTS IF YOU USE ANY LOOPS IN YOUR ANSWER!** Your code should be completely vectorized (a few large computations is faster than many smaller ones)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OZarWwkESM7-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch probability distributions: tensor([[0.0102, 0.0526, 0.0056, 0.9069, 0.0247],\n",
      "        [0.0102, 0.0526, 0.0056, 0.9069, 0.0247]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# for this problem, we'll just copy our old prefix over three times\n",
    "# to form a batch. in practice, each example in the batch would be different.\n",
    "batch_indices = torch.cat(2 * [indices]).reshape((2, 3))\n",
    "batch_embs = embeddings(batch_indices)\n",
    "\n",
    "x = torch.sum(batch_embs,  dim=1)\n",
    "\n",
    "batch_probs = torch.nn.functional.softmax(dim=1, input=torch.matmul(x, W))\n",
    "\n",
    "\n",
    "\n",
    "### DO NOT MODIFY THE BELOW LINE\n",
    "print(\"batch probability distributions:\", batch_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTxHxdG5BpQU"
   },
   "source": [
    "## Question 5 (20 points)\n",
    "\n",
    "Choose  one  paper  from  [ACL 2022](https://aclanthology.org/events/acl-2022/#2022acl-long) that you find interesting. A good way to do this is by scanning the titles and abstracts; there are hundreds of papers so take your time before selecting one!  Then, write a summary in  your own words of the paper you chose. Your summary should answer the following questions: what is its motivation? Why should anyone care about it? Were there things in the paper that you didn't understand at all? What were they? Fill out the cell below, and make sure to write 2-4 paragraphs for the summary to receive full credit! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpg8eU9wNBci"
   },
   "source": [
    "#### <font color=\"red\">Write your answer here (2-4 sentences) </font>\n",
    "\n",
    "**Title of paper**:\n",
    "\n",
    "**Authors**:\n",
    "\n",
    "**URL**:\n",
    "\n",
    "**Your summary**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3vB843_0IFt"
   },
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Select Edit -> Clear All Outputs. This will clear all the outputs from all cells (but will keep the content of all cells). \n",
    "2. Select Runtime -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "3. Once you've rerun everything, select File -> Print --> Save as PDF, or you can also save the webpage as pdf. <font color='blue'> Make sure all your solutions especially the coding parts are displayed in the pdf</font>, it's okay if the provided codes get cut off because lines are not wrapped in code cells).\n",
    "4. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing your graders will see!\n",
    "5. Submit your PDF on Gradescope.\n",
    "\n",
    "\n",
    "####  <font color=\"blue\">  Academic honesty </font>\n",
    "\n",
    "- We  reserve the right to audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your PDF. If you turn in correct answers on your PDF without code that actually generates those answers, we will consider this a serious case of cheating. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### <font color=\"blue\"> Acknowledgements</font>\n",
    "This assignment is based on an assignment developed by Mohit Iyyer"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
